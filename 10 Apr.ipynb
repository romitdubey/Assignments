{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "565c0957-ed5c-4055-90a8-904831d30b72",
   "metadata": {},
   "source": [
    "#### Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191e2ab8-186b-4918-9ce7-07cd035ae0e2",
   "metadata": {},
   "source": [
    "Ans:\n",
    "S be the event that an employee is a smoker,\n",
    "\n",
    "H be the event that an employee uses the company's health insurance plan.\n",
    "\n",
    "P(H) = 0.7 \n",
    "\n",
    "P(S|H) = 0.4\n",
    "\n",
    "P(S|H) = P(H|S) * P(S) / P(H)\n",
    "\n",
    "= 0.4 * 0.7 / P(S)\n",
    "\n",
    "But P(S) = ?\n",
    "\n",
    "Thus, P(S) = P(S|H) * P(H) + P(S|not H) * P(not H)\n",
    "\n",
    "We are not given P(S|not H), but we can assume that P(S|not H) = 0.1\n",
    "\n",
    "= 0.4 * 0.7 + 0.1 * 0.3 = 0.31\n",
    "\n",
    "\n",
    "Finally, \n",
    "\n",
    "P(H|S) = 0.4 * 0.7 / 0.31 = 0.9\n",
    "       \n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13057386-c2b2-4d82-9736-b4a2461c42d6",
   "metadata": {},
   "source": [
    "#### Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62207fad-02e2-4be7-a1cf-27d76ec3b6e9",
   "metadata": {},
   "source": [
    "Ans: \n",
    "- The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes is the type of input they can handle. \n",
    "\n",
    "- Bernoulli Naive Bayes is used for binary input data, where each feature can take only two values (e.g., 0 or 1). \n",
    "\n",
    "- On the other hand, Multinomial Naive Bayes is used for discrete input data, where each feature can take multiple values (e.g., word counts in a document)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0364eb54-ac37-4cb8-a3df-9edec2afcb6e",
   "metadata": {},
   "source": [
    "#### Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba0d6d8-b38c-412a-859e-f834a4fa3de0",
   "metadata": {},
   "source": [
    "Ans: Bernoulli Naive Bayes assumes that the missing values are equal to the complement of the observed values for the feature. \n",
    "\n",
    "For example, if the observed value for a feature is 1, then the missing value is assumed to be 0. \n",
    "\n",
    "Similarly, if the observed value is 0, then the missing value is assumed to be 1. This assumption can be problematic if the missing values are not Missing Completely at Random (MCAR)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63625f84-e106-448d-8608-04fe28746d5f",
   "metadata": {},
   "source": [
    "#### Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ea955ae-843e-466a-9435-6a0d12a8eeee",
   "metadata": {},
   "source": [
    "Ans: \n",
    "- Gaussian Naive Bayes can be used for multi-class classification by using the principle of maximum a posteriori (MAP) estimation. \n",
    "\n",
    "- In this approach, a separate Gaussian distribution is fit to each class, with the mean and variance estimated from the training data for each class. \n",
    "\n",
    "- During prediction, the class with the highest posterior probability is chosen as the predicted class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d662af46-ae3a-4636-aa79-ae5181a7e030",
   "metadata": {},
   "source": [
    "#### Q5. Assignment:\n",
    "\n",
    "#### **Data preparation:**\n",
    "##### Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/ datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message is spam or not based on several input features.\n",
    "\n",
    "#### **Implementation:**\n",
    "##### Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "#### **Results:**\n",
    "##### Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "\n",
    "Precision\n",
    "\n",
    "Recall\n",
    "\n",
    "F1 score\n",
    "\n",
    "#### **Discussion:**\n",
    "##### Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "#### **Conclusion:**\n",
    "##### Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "#### **Note:** This dataset contains a binary classification problem with multiple features. The dataset is relatively small, but it can be used to demonstrate the performance of the different variants of Naive Bayes on a real-world problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c4f5376-cbf8-44b1-90f3-6aaea6bd85dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a51e813a-6a30-4530-8f5e-c94a98baa363",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('spambase.data', delimiter=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d28bc261-a94b-46b2-b285-c1a5d9ea984f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>is_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "0             0.00            0.00  ...         0.00        0.000   \n",
       "1             0.00            0.94  ...         0.00        0.132   \n",
       "2             0.64            0.25  ...         0.01        0.143   \n",
       "3             0.31            0.63  ...         0.00        0.137   \n",
       "4             0.31            0.63  ...         0.00        0.135   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0          0.0        0.778        0.000        0.000   \n",
       "1          0.0        0.372        0.180        0.048   \n",
       "2          0.0        0.276        0.184        0.010   \n",
       "3          0.0        0.137        0.000        0.000   \n",
       "4          0.0        0.135        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  is_spam  \n",
       "0                       278        1  \n",
       "1                      1028        1  \n",
       "2                      2259        1  \n",
       "3                       191        1  \n",
       "4                       191        1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = ['word_freq_make', 'word_freq_address', 'word_freq_all', 'word_freq_3d', 'word_freq_our',\n",
    "                'word_freq_over', 'word_freq_remove', 'word_freq_internet', 'word_freq_order', 'word_freq_mail',\n",
    "                'word_freq_receive', 'word_freq_will', 'word_freq_people', 'word_freq_report', 'word_freq_addresses',\n",
    "                'word_freq_free', 'word_freq_business', 'word_freq_email', 'word_freq_you', 'word_freq_credit',\n",
    "                'word_freq_your', 'word_freq_font', 'word_freq_000', 'word_freq_money', 'word_freq_hp', 'word_freq_hpl',\n",
    "                'word_freq_george', 'word_freq_650', 'word_freq_lab', 'word_freq_labs', 'word_freq_telnet',\n",
    "                'word_freq_857', 'word_freq_data', 'word_freq_415', 'word_freq_85', 'word_freq_technology',\n",
    "                'word_freq_1999', 'word_freq_parts', 'word_freq_pm', 'word_freq_direct', 'word_freq_cs',\n",
    "                'word_freq_meeting', 'word_freq_original', 'word_freq_project', 'word_freq_re', 'word_freq_edu',\n",
    "                'word_freq_table', 'word_freq_conference', 'char_freq_;', 'char_freq_(', 'char_freq_[', 'char_freq_!',\n",
    "                'char_freq_$', 'char_freq_#', 'capital_run_length_average', 'capital_run_length_longest',\n",
    "                'capital_run_length_total', 'is_spam']\n",
    "\n",
    "df.columns = column_names\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54f558e5-d076-4d5b-be7c-2164ac618f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9f4a6f5-62b3-4f6f-b993-97f4171800d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5213b5a5-9f62-4613-879f-da9f3b008767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB:\n",
      "  Accuracy: 0.884\n",
      "  Precision: 0.887\n",
      "  Recall: 0.815\n",
      "  F1 score: 0.848 \n",
      "\n",
      "MultinomialNB:\n",
      "  Accuracy: 0.786\n",
      "  Precision: 0.739\n",
      "  Recall: 0.721\n",
      "  F1 score: 0.728 \n",
      "\n",
      "GaussianNB:\n",
      "  Accuracy: 0.822\n",
      "  Precision: 0.710\n",
      "  Recall: 0.957\n",
      "  F1 score: 0.813 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "for clf in [bernoulli_nb, multinomial_nb, gaussian_nb]:\n",
    "    scores = cross_validate(clf, X, y, cv=10, scoring=scoring)\n",
    "    print(f\"{clf.__class__.__name__}:\")\n",
    "    print(f\"  Accuracy: {scores['test_accuracy'].mean():.3f}\")\n",
    "    print(f\"  Precision: {scores['test_precision'].mean():.3f}\")\n",
    "    print(f\"  Recall: {scores['test_recall'].mean():.3f}\")\n",
    "    print(f\"  F1 score: {scores['test_f1'].mean():.3f}\", '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
