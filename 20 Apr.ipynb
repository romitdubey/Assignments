{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "565c0957-ed5c-4055-90a8-904831d30b72",
   "metadata": {},
   "source": [
    "#### Q1. What is the KNN algorithm?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "043bf6de-e7d0-4034-be96-38083bde66e8",
   "metadata": {},
   "source": [
    "Ans: The **KNN (K-Nearest Neighbors)** algorithm is a type of machine learning algorithm used for both classification and regression. \n",
    "\n",
    "In this algorithm, a data point is classified or predicted based on the majority class or the average of the k-nearest data points in the training dataset. \n",
    "\n",
    "The distance metric used to measure the similarity between the data points can be Euclidean, Manhattan, or other distance measures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13057386-c2b2-4d82-9736-b4a2461c42d6",
   "metadata": {},
   "source": [
    "#### Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a51358d-292f-4df0-ab02-92345f8f3e99",
   "metadata": {},
   "source": [
    "Ans: \n",
    "- The value of k in KNN represents the number of nearest neighbors to consider when classifying or predicting a new data point. The optimal value of k depends on the dataset and the problem at hand. \n",
    "\n",
    "- A small value of k may result in overfitting and high variance, while a large value of k may result in underfitting and high bias. \n",
    "\n",
    "- One common approach to determine the optimal value of k is through cross-validation, where the dataset is split into training and validation sets, and different values of k are tested on the validation set to find the one with the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0364eb54-ac37-4cb8-a3df-9edec2afcb6e",
   "metadata": {},
   "source": [
    "#### Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2698a73-e78d-46d5-be25-c6c4c6d666db",
   "metadata": {},
   "source": [
    "Ans: \n",
    "- The KNN classifier is used for classification tasks, where the goal is to predict the class of a new data point based on the classes of its k-nearest neighbors. \n",
    "\n",
    "- The predicted class is typically the majority class among the k-nearest neighbors. \n",
    "\n",
    "While,\n",
    "\n",
    "- In contrast, the KNN regressor is used for regression tasks, where the goal is to predict the continuous output of a new data point based on the outputs of its k-nearest neighbors. \n",
    "\n",
    "- The predicted output is typically the average of the outputs of the k-nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63625f84-e106-448d-8608-04fe28746d5f",
   "metadata": {},
   "source": [
    "#### Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d796cd9-01af-46fe-a7d2-82eabb77aa9a",
   "metadata": {},
   "source": [
    "Ans: The performance of KNN can be measured using various evaluation metrics, depending on the problem at hand. \n",
    "\n",
    "**For classification tasks**, commonly used metrics include accuracy, precision, recall, F1-score, and ROC AUC. \n",
    "\n",
    "**For regression tasks**, commonly used metrics include mean squared error (MSE), mean absolute error (MAE), and R-squared. \n",
    "\n",
    "Cross-validation and hyperparameter tuning can also be used to optimize the performance of the KNN algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d662af46-ae3a-4636-aa79-ae5181a7e030",
   "metadata": {},
   "source": [
    "#### Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c791ca-2737-4205-8e43-8b28dad55b35",
   "metadata": {},
   "source": [
    "Ans: The *curse of dimensionality in KNN* refers to the problem of increasing feature dimensions in the dataset, where the distance between points becomes less meaningful as the number of dimensions increases. \n",
    "\n",
    "This can lead to sparsity of the data, where the distance between any two points becomes almost equal, making it difficult to distinguish between them. \n",
    "\n",
    "In such cases, dimensionality reduction techniques like principal component analysis (PCA) or feature selection can be applied to reduce the number of features and improve the performance of KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2472a2ee-289c-431c-b53b-0092edc90f05",
   "metadata": {},
   "source": [
    "#### Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1933d4e5-a4bf-4e27-8357-7ae77b114e90",
   "metadata": {},
   "source": [
    "Ans: \n",
    "- One common approach to handling missing values in KNN is to use the imputation technique. \n",
    "\n",
    "- This involves replacing the missing values with estimated values based on the available data. \n",
    "- One approach to imputation is to replace the missing values with the mean or median of the available data in the same feature. \n",
    "- Another approach is to use KNN imputation, where the missing values are estimated based on the values of the k-nearest neighbors in the same feature. \n",
    "- Another technique is to use multiple imputations, where the missing values are estimated multiple times using different imputation methods, and the results are combined to improve the accuracy of the imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9578e651-f40c-46f2-b704-8f5665817405",
   "metadata": {},
   "source": [
    "#### Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee732a2-37c9-4b54-97d5-8168a040d023",
   "metadata": {},
   "source": [
    "Ans: \n",
    "- The **KNN classifier** is better suited for *classification problems*, where the goal is to predict a categorical variable. \n",
    "\n",
    "- The classifier determines the class of a new data point based on the majority class among its k-nearest neighbors. \n",
    "\n",
    "--\n",
    "\n",
    "- The **KNN regressor**, on the other hand, is better suited for *regression problems*, where the goal is to predict a continuous variable. \n",
    "\n",
    "- The regressor predicts the output of a new data point based on the average of the outputs of its k-nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8373255-4233-46a8-bee1-fcaca1cc1daa",
   "metadata": {},
   "source": [
    "#### Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f765affd-896a-4b44-a17d-6331e6a18b20",
   "metadata": {},
   "source": [
    "Ans:\n",
    "- The **strengths** of the KNN algorithm include its simplicity, flexibility, and effectiveness in certain types of datasets. \n",
    "- KNN can work well with nonlinear, non-parametric, and high-dimensional data. \n",
    "\n",
    "--\n",
    "\n",
    "- However, there are also some **weaknesses** to the KNN algorithm. \n",
    "- It can be computationally expensive and sensitive to the choice of distance metric and the value of k. \n",
    "- KNN may also suffer from imbalanced class distributions and the curse of dimensionality. \n",
    "\n",
    "--\n",
    "\n",
    "- To address these weaknesses, techniques such as cross-validation, hyperparameter tuning, feature selection, and dimensionality reduction can be applied to optimize the performance of KNN. \n",
    "- Additionally, ensemble methods like bagging, boosting, and stacking can be used to improve the robustness and accuracy of the KNN algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397fc2f9-2af5-4fe8-b58b-f827b4ee713a",
   "metadata": {},
   "source": [
    "#### Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5689cf-cfc7-44c1-962a-a548f4db6f86",
   "metadata": {},
   "source": [
    "Ans: Euclidean distance and Manhattan distance are two commonly used distance metrics in KNN. \n",
    "\n",
    "**Euclidean distance** measures the straight-line distance between two points in a Euclidean space, while\n",
    "\n",
    "**Manhattan distance** measures the distance between two points along the axes of a grid. \n",
    "\n",
    "--\n",
    "\n",
    "- In Euclidean distance, the sum of the squared differences between the coordinates of the two points is calculated, and the square root of the sum is taken to get the final distance. \n",
    "\n",
    "- In Manhattan distance, the sum of the absolute differences between the coordinates of the two points is calculated. \n",
    "\n",
    "--\n",
    "\n",
    "- The choice between Euclidean and Manhattan distance depends on the nature of the data and the problem at hand. \n",
    "\n",
    "- Euclidean distance is more sensitive to differences in continuous variables, while Manhattan distance is more robust to outliers and differences in categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d991fa4-2c86-4b57-8751-3108c89fd0e5",
   "metadata": {},
   "source": [
    "#### Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ecab68-4899-4d62-bbfc-d704bdf3f332",
   "metadata": {},
   "source": [
    "Ans: Feature scaling is the process of normalizing the range of features to a common scale in order to prevent the dominance of certain features over others in the KNN algorithm. \n",
    "\n",
    "Since KNN relies on calculating distances between data points, the features with larger ranges or scales may dominate the distance calculations and bias the algorithm towards those features. \n",
    "\n",
    "Feature scaling can be done using techniques like standardization or min-max scaling. Standardization scales the features to have a mean of zero and a standard deviation of one, while min-max scaling scales the features to a range between zero and one. \n",
    "\n",
    "By scaling the features, the KNN algorithm can be *more accurate and reliable*, and the distances between data points can be more meaningful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
